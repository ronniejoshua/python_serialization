"""
Why do we need serialization?
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] Serialization is the act of taking Python objects, such as dict list, daytime and others, and converting them to a sequence of bytes. On the other side, there's de-serialization, we'll take a sequence of bytes and convert them to a Python objects or several objects. The fist question people ask is why, why do we need to do that? The answer is that internally computer knows only about bits of zeros and ones to help us humans, we divided these bits into groups of eight and call them bytes. The Python interpreter, obstruct this for you and let you work at the higher level thinking of sets, lists, strings, and other data types. However, when you need to transfer or store data, it must be converted to a sequence of bytes. The second question is how, how can you convert an object such as a Python list to a sequence of bytes? There are many answers to these questions. These answers have names such as JSON, YAML, PICKLE, XML, and more. Each of these names is the name of a serialization format. There are many serialization formats out there and we'll discuss the pros and cons of some of the major ones throughout this course. Serialization is usually done at the edges of your program. When you receive data from a socket, write to a file, create a database, et cetera. Inside your program, you will work with high level Python objects. The usual workflow is this serialized bytes to Python object or objects from the input work with Python objects and finally serialize Python, object or objects into bytes to Dell.
"""

"""
Picking a serialization format
Selecting transcript lines in this section will navigate to timestamp in the video
There are many serialization formats out there from familiar names such as JSON and XML to less familiar, such as Captain proto, and others. In this video, we'll discuss some of the parameters you need to consider when picking a serialization format. The first thing you should consider is how mature is the format. I love this equation by Martin Wiener. Maturity is blood plus sweat divided by complexity. These days most projects are hosted on GitHub, which makes it easy to check parameters such as how old is the project, how many contributes, does it have, how many open bugs, etc. In general, ask around. Don't get tempted by the shiny new things go for old and boring technologies. Another critter is how many programming languages support this format. This might not be interesting if you write everything in the same language, and mostly communicate internally. However, for external API's, this might be very important. JSON for example, is supported by many programming languages. It's one of the reasons why it's very popular in API's. Another criterion is the type supported by the format. JSON does not have daytime or timestamp format. When passing time information, you will need to convert time to a string or a number. Protocol Buffers does have a timestamp format. And if you use it, you will avoid the need to convert your daytime objects to strings or numbers. Schema is also an important factor. Schema defines how your messages look like for example, it can say that the log message have a time field which is a timestamp. A level field, which is an integer and a message field, which is a string schema is make sure your data is correct and that services agree on what is being sent and received. On the flip side schemas makes it harder to change data format. Ask anyone who's done SQL schema migration how painful it was. In general, I prefer formats with schema over schema less ones. Schema helped me validate the data and detect errors. It also make me think about how the data should be structured. In the long run, it pays out in spades. Performance might be a consideration, make sure to have performance requirements before you select the format. modern computers are very fast, and most from us will be good enough for your needs. By performance, I mean both CPU how much time it takes to serialize and deserialize the data and size, how many bytes are sent or stored. Both CPU time and bandwidth, the size, cost, money and picking the right serialization format can save you a lot of money. Make sure to run benchmarks against a couple of formats on real data before picking one. The last criterion I'll mentioned here is the standard library. Installing third party packages carries a risk and operational complexity. If you use the format that's already in the standard library, such as Jason, you don't need to install anything. And you know, it's well debunked. There are many other criterion you might have want to consider such as security streaming and others. Please, please don't invent your own. There are many formats out there. We don't need another one. The main point is that you should be conscious about how to pick a format. Don't reach to the first thing that comes to your mind, or is considered cool today. So which one should you pick? I can't tell you. It really depends on your use case. What I can say is that most companies I consult with, here's JSON for external API's and Protocol Buffers for internal communication between services, but don't be lazy. Do your homework.
"""

"""
General serialization rules
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] No matter which serialization format you pick, there are some practices you should follow. The first thing you should do even before picking a format is design and document your data. You need to think about what are the objects being passed? What are their properties and what are the limitation on the data? Say you're passing a weather measurement. What are the fields? What is the units of the temperature? Et cetera, et cetera. Well designed data makes it easier to understand code and validate. Don't skip this step and make sure to review the design on every update. The second rule is that you should serialize only at the edges of your program. Say you get a timestamp as a string. You should convert it to a Python datetime object and work with this object internally. I've seen people pass around the timestamp string and deserialize it over and over again. When you Serialize at the Edges, you validate the data at one place and internal components can assume the data is valid, leading to shorter and more efficient code. The third rule is to think about security. Some serialization formats are more secure than others, but no serialization format is totally secure. For example, a few years back, it was enough to send you a JPEG image to gain control over your computer. This was due to a bug in the JPEG passing code that allowed people to construct malicious images. There are zip bombs, XML billion laughs, and other security risks. You should be aware of them and never trust data coming from the outside. The fourth rule is to know the format you're working with. Every format has its own quirks. And by knowing them you'll avoid long hours of chasing bugs. For example, Jason does not have a topple tap, which means that when you serialize a topple to Jason, he will get back at least when you deserialize this might be surprising and even lead to bugs. If you depend on topples immutability. The last thing I'll mention is decoupling of internal data from serialization. Most projects start by serializing the internal data structure say a log message. But when they want to change the internal implementation, they have a problem. You can't change your public API whenever you want. It's a long and painful process. Since this decoupling will eventually happen. Plan for it and design your external APIs in a way that is future.
"""


"""
Serialization formats overview
Selecting transcript lines in this section will navigate to timestamp in the video
- [Instructor] In this video, we'll go over some of the common serialization formats and discuss some of their properties. It won't be an extensive list, but the major players will make an appearance. In all of the examples, we look at the same data, which is a metric center server, such as Prometheus or Influx DB. It has time, which is a date-time sometimes called a timestamp, a name, which is a string, a value, which is a float, and labels, which is a dict. Pickle is a Python-only format. It can serialize almost any Python type, including your custom classes. Pickle is great for Python to Python communication. You don't need to worry about which types are supported. I know several large companies who use Pickle for internal communication. Since Pickle might use the built in evol function, during de-serialization, you need to be security-aware when using it. JSON is by far the most popular sterilization format out there. It is text-based and support a limited set of types. As you can see, the time field is a string, here. Another major limitation of JSON is that there are no comments, making it unusable for configuration. Trust me, you don't want configuration files without comments in them. JSON looks like Python and some people confuse them. Don't do the same mistake. YAML and TOML are two text-based formats that are very popular in configuration. They support a wider variety of types than JSON and have comments. The YAML format is more widespread due to its adoption in the operations and Kubernetes faults. YAML can get complex and the serialization time is usually slow, which, for configuration, is not an issue. TOML is a newer player. It's been adopted in the Python world in places such as pyproject.TOML and others. CSV and XML are old and established formats. They are not cool as JSON or TOML, but at one point or another, you will work with them. CSV's main advantage is that you can import and export it from Excel. There's no schema specification for CSV, and everything you read or write to CSV must be a string. You will need the external knowledge that the value is a float. Some popular libraries such as pandas will guess the type for you. Note we didn't quote the labels since CSV does not support hierarchical data. In XML, everything is a string, as well. There are external schemas or DTDs where you can define types. The Python's TOML library has models for working with CSV and XML. Msgpack and BSON are binary formats without schema. You can think of them as binary JSON. BSON is the internal format Mongo DB uses, meaning it's widely used and debugged. Msgpack is open format and supports a wider range of programming languages. Protocol buffers are a format that started at Google and became very popular. In protocol buffers, also called protobuf, you wrote a .proto file where you define your datatypes called messages, then you use the protoc compiler to generate serialization code for various languages. The big plus is that you have one definition for your messages, for all the languages using it. Another plus, it's a binary format and very efficient. Protocol buffers is mostly used for internal communication between services. SQL is the format to define data in relational databases. Its old and established format and well-suited for querying data. SQL has a schema, it practically invented it, and supports a wide range of types. In the Python standard library, you'll find the built-in SQL3 model. I've used it many times and it's great. Parquet, ORC, HDF5, Arrow and more, these formats are great for storing a lot of data. You can find the code that generated these examples in the exercise files for this chapter.
"""